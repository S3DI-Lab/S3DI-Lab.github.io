// A fundamental tradeoff between computation and communication in distributed computing
@ARTICLE{8051074,
  author={Li, Songze and Maddah-Ali, Mohammad Ali and Yu, Qian and Avestimehr, A. Salman},
  journal={IEEE Transactions on Information Theory}, 
  title={A Fundamental Tradeoff Between Computation and Communication in Distributed Computing}, 
  year={2018},
  volume={64},
  number={1},
  pages={109-128},
  keywords={Encoding;Distributed databases;Benchmark testing;Electrical engineering;Electronic mail;Distributed computing;MapReduce;computation-communication tradeoff;coded multicasting;coded TeraSort},
  doi={10.1109/TIT.2017.2756959},
  bibtex_show={true},
  pdf={https://arxiv.org/pdf/1604.07086},
  abbr={TIT 2018},
  }

// Lagrange coded computing: Optimal design for resiliency, security, and privacy
@InProceedings{pmlr-v89-yu19b,
  title = 	 {Lagrange Coded Computing: Optimal Design for Resiliency, Security, and Privacy},
  author =       {Yu, Qian and Li, Songze and Raviv, Netanel and Kalan, Seyed Mohammadreza Mousavi and Soltanolkotabi, Mahdi and Avestimehr, Salman A.},
  booktitle = 	 {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1215--1225},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v89/yu19b/yu19b.pdf},
  url = 	 {https://proceedings.mlr.press/v89/yu19b.html},
  abstract = 	 {We consider a  scenario involving computations over a massive dataset stored distributedly across multiple workers, which is at the core of distributed learning algorithms. We propose Lagrange Coded Computing (LCC), a new framework to simultaneously provide (1) resiliency against stragglers that may prolong computations; (2) security against Byzantine (or malicious) workers that deliberately modify the computation for their benefit; and (3) (information-theoretic) privacy of the dataset amidst possible collusion of workers. LCC, which leverages the well-known Lagrange polynomial to create computation redundancy in a novel coded form across workers, can be applied to any computation scenario in which the function of interest is an arbitrary multivariate polynomial of the input dataset, hence covering many computations of interest in machine learning. LCC significantly generalizes prior works to go beyond linear computations. It also enables secure and private computing in distributed settings, improving the computation and communication efficiency of the state-of-the-art. Furthermore, we prove the optimality of LCC by showing that it achieves the optimal tradeoff between resiliency, security, and privacy, i.e., in terms of tolerating the maximum number of stragglers and adversaries, and providing data privacy against the maximum number of colluding workers. Finally, we show via experiments on Amazon EC2 that LCC speeds up the conventional uncoded implementation of distributed least-squares linear regression by up to $13.43\times$, and also achieves a $2.36\times$-$12.65\times$ speedup over the state-of-the-art straggler mitigation strategies.},
  bibtex_show = {true},
  pdf={http://proceedings.mlr.press/v89/yu19b/yu19b.pdf},
  abbr={PMLR 2019}
}

// Fedml: A research library and benchmark for federated machine learning
@misc{he2020fedmlresearchlibrarybenchmark,
      title={FedML: A Research Library and Benchmark for Federated Machine Learning}, 
      author={Chaoyang He and Songze Li and Jinhyun So and Xiao Zeng and Mi Zhang and Hongyi Wang and Xiaoyang Wang and Praneeth Vepakomma and Abhishek Singh and Hang Qiu and Xinghua Zhu and Jianzong Wang and Li Shen and Peilin Zhao and Yan Kang and Yang Liu and Ramesh Raskar and Qiang Yang and Murali Annavaram and Salman Avestimehr},
      year={2020},
      eprint={2007.13518},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2007.13518}, 
      bibtex_show={true},
      pdf={https://arxiv.org/pdf/2007.13518},
      abbr={arXiv}
}

// A unified coding framework for distributed computing with straggling servers
@INPROCEEDINGS{7848828,
  author={Li, Songze and Maddah-Ali, Mohammad Ali and Avestimehr, A. Salman},
  booktitle={2016 IEEE Globecom Workshops (GC Wkshps)}, 
  title={A Unified Coding Framework for Distributed Computing with Straggling Servers}, 
  year={2016},
  volume={},
  number={},
  pages={1-6},
  keywords={Servers;Encoding;Bandwidth;Multicast communication;Computational modeling;Electrical engineering},
  doi={10.1109/GLOCOMW.2016.7848828},
  bibtex_show = {true},
  pdf={https://arxiv.org/pdf/1609.01690},
  abbr={GC Wkshps2016}
  }

// Coded mapreduce
@INPROCEEDINGS{7447112,
  author={Li, Songze and Maddah-Ali, Mohammad Ali and Avestimehr, A. Salman},
  booktitle={2015 53rd Annual Allerton Conference on Communication, Control, and Computing (Allerton)}, 
  title={Coded MapReduce}, 
  year={2015},
  volume={},
  number={},
  pages={964-971},
  keywords={Servers;Encoding;Electrical engineering;Cache memory;Local area networks;Multicast communication;Programming},
  doi={10.1109/ALLERTON.2015.7447112},
  bibtex_show = {true},
  pdf={https://arxiv.org/pdf/1512.01625},
  abbr={arXiv}
  }

// Coding for distributed fog computing
@ARTICLE{7901473,
  author={Li, Songze and Maddah-Ali, Mohammad Ali and Avestimehr, A. Salman},
  journal={IEEE Communications Magazine}, 
  title={Coding for Distributed Fog Computing}, 
  year={2017},
  volume={55},
  number={4},
  pages={34-40},
  keywords={Bandwidth;Encoding;Edge computing;Redundancy;Time factors;Encoding;Computer architecture},
  doi={10.1109/MCOM.2017.1600894},
  bibtex_show = {true},
  pdf={https://arxiv.org/pdf/1702.06082},
  abbr={IEEE Comm. Mag.}
  }

// Jinhyun So
@article{he2020jinhyun,
  title={Jinhyun So},
  author={He, Chaoyang and Li, Songze},
  journal={Mi Zhang, Hongyi Wang, Xiaoyang Wang, Praneeth Vepakomma, Abhishek Singh, Hang Qiu, Li Shen, Peilin Zhao, Yan Kang, Yang Liu, Ramesh Raskar, Qiang Yang, Murali Annavaram, and Salman Avestimehr. Fedml: A research library and benchmark for federated machine learning},
  pages={33},
  year={2020},
  bibtex_show={true},
}

// Polyshard: Coded sharding achieves linearly scaling efficiency and security simultaneously
@ARTICLE{9141331,
  author={Li, Songze and Yu, Mingchao and Yang, Chien-Sheng and Avestimehr, Amir Salman and Kannan, Sreeram and Viswanath, Pramod},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={PolyShard: Coded Sharding Achieves Linearly Scaling Efficiency and Security Simultaneously}, 
  year={2021},
  volume={16},
  number={},
  pages={249-261},
  keywords={Throughput;Scalability;Encoding;Secure storage;Time measurement;Scalability;blockchain;security and trust;decentralized networks;coded sharding;information theory},
  doi={10.1109/TIFS.2020.3009610},
  bibtex_show={true},
  pdf={https://arxiv.org/pdf/1809.10361},
  abbr={TIFS2021}
}

// A scalable framework for wireless distributed computing
@ARTICLE{7935426,
  author={Li, Songze and Yu, Qian and Maddah-Ali, Mohammad Ali and Avestimehr, A. Salman},
  journal={IEEE/ACM Transactions on Networking}, 
  title={A Scalable Framework for Wireless Distributed Computing}, 
  year={2017},
  volume={25},
  number={5},
  pages={2643-2654},
  keywords={Wireless communication;Distributed computing;Encoding;Mobile communication;Uplink;Downlink;Communication system security;Wireless distributed computing;edge computing;coding;information theory;scalability},
  doi={10.1109/TNET.2017.2702605},
  bibtex_show={true},
  pdf={https://arxiv.org/pdf/1608.05743},
  abbr={TON2017}
  }

// Pipe-SGD: A Decentralized Pipelined SGD Framework for Distributed Deep Net Training
@inproceedings{NEURIPS2018_2c6a0bae,
 author = {Li, Youjie and Yu, Mingchao and Li, Songze and Avestimehr, Salman and Kim, Nam Sung and Schwing, Alexander},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Pipe-SGD: A Decentralized Pipelined SGD Framework for Distributed Deep Net Training},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/2c6a0bae0f071cbbf0bb3d5b11d90a82-Paper.pdf},
 volume = {31},
 year = {2018},
 bibtex_show={true},
 pdf={https://proceedings.neurips.cc/paper_files/paper/2018/file/2c6a0bae0f071cbbf0bb3d5b11d90a82-Paper.pdf},
 abbr={NIPS2018}
}

// LightSecAgg: a Lightweight and Versatile Design for Secure Aggregation in Federated Learning
@inproceedings{MLSYS2022_6c44dc73,
 author = {So, Jinhyun and He, Chaoyang and Yang, Chien-Sheng and Li, Songze and Yu, Qian and E. Ali, Ramy and Guler, Basak and Avestimehr, Salman},
 booktitle = {Proceedings of Machine Learning and Systems},
 editor = {D. Marculescu and Y. Chi and C. Wu},
 pages = {694--720},
 title = {LightSecAgg: a Lightweight and Versatile Design for Secure Aggregation in Federated Learning},
 url = {https://proceedings.mlsys.org/paper_files/paper/2022/file/6c44dc73014d66ba49b28d483a8f8b0d-Paper.pdf},
 volume = {4},
 year = {2022},
 bibtex_show = {true},
 pdf={https://proceedings.mlsys.org/paper_files/paper/2022/file/6c44dc73014d66ba49b28d483a8f8b0d-Paper.pdf},
 abbr={Proc. Mach. Learn. Syst.}
}

// Near-Optimal Straggler Mitigation for Distributed Gradient Methods
@INPROCEEDINGS{8425504,
  author={Li, Songze and Mousavi Kalan, Seyed Mohammadreza and Avestimehr, A. Salman and Soltanolkotabi, Mahdi},
  booktitle={2018 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)}, 
  title={Near-Optimal Straggler Mitigation for Distributed Gradient Methods}, 
  year={2018},
  volume={},
  number={},
  pages={857-866},
  keywords={Training;Computational modeling;Encoding;Task analysis;Distributed databases;Data models;Distributed gradient methods;Straggler mitigation;Near optimal recovery threshold;Near optimal communication load;Coupon's collector;Heterogeneous cluster},
  doi={10.1109/IPDPSW.2018.00137},
  bibtex_show = {true},
  pdf={https://arxiv.org/pdf/1710.09990},
  abbr={IPDPSW2018}
  }

// Coded merkle tree: Solving data availability attacks in blockchains
@InProceedings{10.1007/978-3-030-51280-4_8,
author="Yu, Mingchao
and Sahraei, Saeid
and Li, Songze
and Avestimehr, Salman
and Kannan, Sreeram
and Viswanath, Pramod",
editor="Bonneau, Joseph
and Heninger, Nadia",
title="Coded Merkle Tree: Solving Data Availability Attacks in Blockchains",
booktitle="Financial Cryptography and Data Security",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="114--134",
abstract="In this paper, we propose coded Merkle tree (CMT), a novel hash accumulator that offers a constant-cost protection against data availability attacks in blockchains, even if the majority of the network nodes are malicious. A CMT is constructed using a family of sparse erasure codes on each layer, and is recovered by iteratively applying a peeling-decoding technique that enables a compact proof for data availability attack on any layer. Our algorithm enables any node to verify the full availability of any data block generated by the system by just downloading a {\$}{\$}{\backslash}varTheta (1){\$}{\$}byte block hash commitment and randomly sampling {\$}{\$}{\backslash}varTheta ({\backslash}log b){\$}{\$}bytes, where b is the size of the data block. With the help of only one connected honest node in the system, our method also allows any node to verify any tampering of the coded Merkle tree by just downloading {\$}{\$}{\backslash}varTheta ({\backslash}log b){\$}{\$}bytes. We provide a modular library for CMT in Rust and Python and demonstrate its efficacy inside the Parity Bitcoin client.",
isbn="978-3-030-51280-4",
bibtex_show = {true},
pdf={https://arxiv.org/pdf/1910.01247},
abbr={FC2020}
}

// Gradiveq: Vector quantization for bandwidth-efficient gradient aggregation in distributed cnn training
@inproceedings{NEURIPS2018_cf059682,
 author = {Yu, Mingchao and Lin, Zhifeng and Narra, Krishna and Li, Songze and Li, Youjie and Kim, Nam Sung and Schwing, Alexander and Annavaram, Murali and Avestimehr, Salman},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {GradiVeQ: Vector Quantization for Bandwidth-Efficient Gradient Aggregation in Distributed CNN Training},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/cf05968255451bdefe3c5bc64d550517-Paper.pdf},
 volume = {31},
 year = {2018},
 bibtex_show={true},
 pdf={https://proceedings.neurips.cc/paper_files/paper/2018/file/cf05968255451bdefe3c5bc64d550517-Paper.pdf},
  abbr={NIPS2018}
}

// Coded Computing: Mitigating Fundamental Bottlenecks in Large-Scale Distributed Computing and Machine Learning
@article{CIT-103,
url = {http://dx.doi.org/10.1561/0100000103},
year = {2020},
volume = {17},
journal = {Foundations and Trends® in Communications and Information Theory},
title = {Coded Computing: Mitigating Fundamental Bottlenecks in Large-Scale Distributed Computing and Machine Learning},
doi = {10.1561/0100000103},
issn = {1567-2190},
number = {1},
pages = {1-148},
author = {Songze Li and Salman Avestimehr},
bibtex_show={true},
abbr={Found. Trends Commun. Inf. Theory}
}

// Coded terasort
@INPROCEEDINGS{7965073,
  author={Li, Songze and Supittayapornpong, Sucha and Maddah-Ali, Mohammad Ali and Avestimehr, Salman},
  booktitle={2017 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)}, 
  title={Coded TeraSort}, 
  year={2017},
  volume={},
  number={},
  pages={389-398},
  keywords={Encoding;Sorting;Machine learning algorithms;Distributed databases;Redundancy;Benchmark testing;Distributed Computing;Machine Learning;Sorting;MapReduce;Data Shuffling;Coding},
  doi={10.1109/IPDPSW.2017.33},
  bibtex_show = {true},
  pdf={https://arxiv.org/pdf/1702.04850},
  abbr={IPDPSW2017}
  }

// How to optimally allocate resources for coded distributed computing?
@INPROCEEDINGS{7996730,
  author={Yu, Qian and Li, Songze and Maddah-Ali, Mohammad Ali and Avestimehr, A. Salman},
  booktitle={2017 IEEE International Conference on Communications (ICC)}, 
  title={How to optimally allocate resources for coded distributed computing?}, 
  year={2017},
  volume={},
  number={},
  pages={1-7},
  keywords={Servers;Resource management;Distributed databases;Encoding;Cloud computing;Distributed processing},
  doi={10.1109/ICC.2017.7996730},
  bibtex_show={true},
  pdf={https://arxiv.org/pdf/1702.07297},
  abbr={ICC2017}
  }

// Coded Distributed Computing: Straggling Servers and Multistage Dataflows
@INPROCEEDINGS{7852225,
  author={Li, Songze and Maddah-Ali, Mohammad Ali and Avestimehr, A. Salman},
  booktitle={2016 54th Annual Allerton Conference on Communication, Control, and Computing (Allerton)}, 
  title={Coded Distributed Computing: Straggling Servers and Multistage Dataflows}, 
  year={2016},
  volume={},
  number={},
  pages={164-171},
  keywords={Servers;Encoding;Bandwidth;Computational modeling;Distributed databases;Electrical engineering},
  doi={10.1109/ALLERTON.2016.7852225},
  bibtex_show = {true},
  abbr={Allerton2016}
  }

//Polynomially Coded Regression: Optimal Straggler Mitigation via Data Encoding
@article{DBLP:journals/corr/abs-1805-09934,
  author       = {Songze Li and
                  Seyed Mohammadreza Mousavi Kalan and
                  Qian Yu and
                  Mahdi Soltanolkotabi and
                  Amir Salman Avestimehr},
  title        = {Polynomially Coded Regression: Optimal Straggler Mitigation via Data
                  Encoding},
  journal      = {CoRR},
  volume       = {abs/1805.09934},
  year         = {2018},
  url          = {http://arxiv.org/abs/1805.09934},
  eprinttype    = {arXiv},
  eprint       = {1805.09934},
  timestamp    = {Wed, 21 Oct 2020 19:29:39 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1805-09934.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  bibtex_show={true},
  pdf={https://arxiv.org/pdf/1805.09934},
  abbr={arXiv}
}

// Edge-Facilitated Wireless Distributed Computing
@INPROCEEDINGS{7841765,
  author={Li, Songze and Yu, Qian and Maddah-Ali, Mohammad Ali and Avestimehr, A. Salman},
  booktitle={2016 IEEE Global Communications Conference (GLOBECOM)}, 
  title={Edge-Facilitated Wireless Distributed Computing}, 
  year={2016},
  volume={},
  number={},
  pages={1-7},
  keywords={Distributed computing;Wireless communication;Downlink;Uplink;Computational modeling;Mobile communication;Encoding},
  doi={10.1109/GLOCOM.2016.7841765},
  bibtex_show = {true},
  abbr={GLOBECOM2016}
  }

// SwiftAgg+: Achieving Asymptotically Optimal Communication Loads in Secure Aggregation for Federated Learning
@ARTICLE{10058697,
  author={Jahani-Nezhad, Tayyebeh and Maddah-Ali, Mohammad Ali and Li, Songze and Caire, Giuseppe},
  journal={IEEE Journal on Selected Areas in Communications}, 
  title={SwiftAgg+: Achieving Asymptotically Optimal Communication Loads in Secure Aggregation for Federated Learning}, 
  year={2023},
  volume={41},
  number={4},
  pages={977-989},
  keywords={Servers;Load modeling;Computational modeling;Cryptography;Data models;Symbols;Protocols;Federated learning;secure aggregation;secret sharing;dropout resiliency;optimal communication load},
  doi={10.1109/JSAC.2023.3242702},
  bibtex_show={true},
  pdf={https://arxiv.org/pdf/2203.13060},
  abbr={JSAC2023}
  }

// Compressed Coded Distributed Computing
@INPROCEEDINGS{8437882,
  author={Li, Songze and Maddah-Ali, Mohammad Ali and Avestimehr, A. Salman},
  booktitle={2018 IEEE International Symposium on Information Theory (ISIT)}, 
  title={Compressed Coded Distributed Computing}, 
  year={2018},
  volume={},
  number={},
  pages={2032-2036},
  keywords={},
  doi={10.1109/ISIT.2018.8437882},
  bibtex_show = {true},
  pdf={https://arxiv.org/pdf/1805.01993},
  abbr={ISIT2018}
  }

// Lightsecagg: Rethinking secure aggregation in federated learning
@article{yang2021lightsecagg,
  title={Lightsecagg: Rethinking secure aggregation in federated learning},
  author={Yang, Chien-Sheng and So, Jinhyun and He, Chaoyang and Li, Songze},
  year={2021}
}

// Communication-aware computing for edge processing
@INPROCEEDINGS{8007057,
  author={Li, Songze and Maddah-Ali, Mohammad Ali and Avestimehr, A. Salman},
  booktitle={2017 IEEE International Symposium on Information Theory (ISIT)}, 
  title={Communication-aware computing for edge processing}, 
  year={2017},
  volume={},
  number={},
  pages={2885-2889},
  keywords={Mobile communication;Edge computing;Interference;Information theory;Mobile computing;Base stations;Smart phones},
  doi={10.1109/ISIT.2017.8007057},
  bibtex_show = {true},
  pdf={https://arxiv.org/pdf/1706.07523},
  abbr={ISIT2017}
  }

// DReS-FL: Dropout-Resilient Secure Federated Learning for Non-IID Clients via Secret Data Sharing
@inproceedings{NEURIPS2022_448fc91f,
 author = {Shao, Jiawei and Sun, Yuchang and Li, Songze and Zhang, Jun},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {10533--10545},
 publisher = {Curran Associates, Inc.},
 title = {DReS-FL: Dropout-Resilient Secure Federated Learning for Non-IID Clients via Secret Data Sharing},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/448fc91f669c15d10364ee01d512cc10-Paper-Conference.pdf},
 volume = {35},
 year = {2022},
 bibtex_show={true},
 pdf={https://proceedings.neurips.cc/paper_files/paper/2022/file/448fc91f669c15d10364ee01d512cc10-Paper-Conference.pdf},
 abbr={NIPS2022}
}

// SwiftAgg: Communication-Efficient and Dropout-Resistant Secure Aggregation for Federated Learning with Worst-Case Security Guarantees
@INPROCEEDINGS{9834750,
  author={Jahani-Nezhad, Tayyebeh and Maddah-Ali, Mohammad Ali and Li, Songze and Caire, Giuseppe},
  booktitle={2022 IEEE International Symposium on Information Theory (ISIT)}, 
  title={SwiftAgg: Communication-Efficient and Dropout-Resistant Secure Aggregation for Federated Learning with Worst-Case Security Guarantees}, 
  year={2022},
  volume={},
  number={},
  pages={103-108},
  keywords={Protocols;Distributed databases;Collaborative work;Data models;Servers;Cryptography;Communication networks;Federated learning;Communication-efficient secure aggregation;Secret sharing;Dropout resiliency},
  doi={10.1109/ISIT50566.2022.9834750},
  bibtex_show=true,
  pdf={https://arxiv.org/pdf/2202.04169},
  abbr={ISIT2022}
  }

// Coded distributed computing: Fundamental limits and practical challenges
@INPROCEEDINGS{7869092,
  author={Li, Songze and Yu, Qian and Maddah-Ali, Mohammad Ali and Avestimehr, A. Salman},
  booktitle={2016 50th Asilomar Conference on Signals, Systems and Computers}, 
  title={Coded distributed computing: Fundamental limits and practical challenges}, 
  year={2016},
  volume={},
  number={},
  pages={509-513},
  keywords={Encoding;Sorting;Distributed databases;Bandwidth;Servers;Approximation algorithms},
  doi={10.1109/ACSSC.2016.7869092},
  bibtex_show = {true},
  abbr={Conf. Signals Syst. Comput.}
  }

// Stochastic Coded Federated Learning with Convergence and Privacy Guarantees
@INPROCEEDINGS{9834445,
  author={Sun, Yuchang and Shao, Jiawei and Li, Songze and Mao, Yuyi and Zhang, Jun},
  booktitle={2022 IEEE International Symposium on Information Theory (ISIT)}, 
  title={Stochastic Coded Federated Learning with Convergence and Privacy Guarantees}, 
  year={2022},
  volume={},
  number={},
  pages={2028-2033},
  keywords={Training;Privacy;Differential privacy;Computational modeling;Stochastic processes;Machine learning;Collaborative work;Federated learning (FL);coded computing;stochastic gradient descent (SGD);mutual information differential privacy (MI-DP)},
  doi={10.1109/ISIT50566.2022.9834445},
  bibtex_show = {true},
  pdf={https://arxiv.org/pdf/2201.10092},
  abbr={ISIT2022}
  }

// OmniLytics: {A} Blockchain-based Secure Data Market for Decentralized Machine Learning
@article{DBLP:journals/corr/abs-2107-05252,
  author       = {Jiacheng Liang and
                  Wensi Jiang and
                  Songze Li},
  title        = {OmniLytics: {A} Blockchain-based Secure Data Market for Decentralized
                  Machine Learning},
  journal      = {CoRR},
  volume       = {abs/2107.05252},
  year         = {2021},
  url          = {https://arxiv.org/abs/2107.05252},
  eprinttype    = {arXiv},
  eprint       = {2107.05252},
  timestamp    = {Tue, 20 Jul 2021 15:08:33 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2107-05252.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  bibtex_show = {true},
  pdf={https://arxiv.org/pdf/2107.05252},
  abbr={arXiv}
}

// Symmetric Private Polynomial Computation From Lagrange Encoding
@ARTICLE{9672125,
  author={Zhu, Jinbao and Yan, Qifa and Tang, Xiaohu and Li, Songze},
  journal={IEEE Transactions on Information Theory}, 
  title={Symmetric Private Polynomial Computation From Lagrange Encoding}, 
  year={2022},
  volume={68},
  number={4},
  pages={2704-2718},
  keywords={Servers;Systematics;Encoding;Decoding;Costs;Computational complexity;Codes;Private computation;symmetric private polynomial computation;Lagrange encoding;computation complexity},
  doi={10.1109/TIT.2022.3140890},
  bibtex_show = {true},
  pdf={https://arxiv.org/pdf/2010.09326},
  abbr={TIT2022}
}

// Chameleon: Adapting to Peer Images for Planting Durable Backdoors in Federated Learning
@InProceedings{pmlr-v202-dai23a,
  title = 	 {Chameleon: Adapting to Peer Images for Planting Durable Backdoors in Federated Learning},
  author =       {Dai, Yanbo and Li, Songze},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {6712--6725},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 July},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/dai23a/dai23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/dai23a.html},
  abstract = 	 {In a federated learning (FL) system, distributed clients upload their local models to a central server to aggregate into a global model. Malicious clients may plant backdoors into the global model through uploading poisoned local models, causing images with specific patterns to be misclassified into some target labels. Backdoors planted by current attacks are not durable, and vanish quickly once the attackers stop model poisoning. In this paper, we investigate the connection between the durability of FL backdoors and the relationships between benign images and poisoned images (i.e., the images whose labels are flipped to the target label during local training). Specifically, benign images with the original and the target labels of the poisoned images are found to have key effects on backdoor durability. Consequently, we propose a novel attack, Chameleon, which utilizes contrastive learning to further amplify such effects towards a more durable backdoor. Extensive experiments demonstrate that Chameleon significantly extends the backdoor lifespan over baselines by $1.2\times \sim 4\times$, for a wide range of image datasets, backdoor types, and model architectures.},
  bibtex_show = {true},
  pdf={https://proceedings.mlr.press/v202/dai23a/dai23a.pdf},
  abbr={ICML2023}
}

// A Systematic Approach Towards Efficient Private Matrix Multiplication
@ARTICLE{9790818,
  author={Zhu, Jinbao and Li, Songze},
  journal={IEEE Journal on Selected Areas in Information Theory}, 
  title={A Systematic Approach Towards Efficient Private Matrix Multiplication}, 
  year={2022},
  volume={3},
  number={2},
  pages={257-274},
  keywords={Codes;Costs;Security;Indexes;Data privacy;Computational complexity;Task analysis;Coded distributed computing;fully private matrix multiplication;lagrange codes;polynomial codes;private and secure matrix multiplication;secure matrix multiplication},
  doi={10.1109/JSAIT.2022.3181144},
  bibtex_show = {true},
  pdf={https://arxiv.org/pdf/2201.00646},
  abbr={JSIT2022}
  }

// Proceedings of the 40th International Conference on Machine Learning
@InProceedings{pmlr-v202-li23an,
  title = 	 {{F}ed{VS}: Straggler-Resilient and Privacy-Preserving Vertical Federated Learning for Split Models},
  author =       {Li, Songze and Yao, Duanyi and Liu, Jin},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {20296--20311},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 July},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/li23an/li23an.pdf},
  url = 	 {https://proceedings.mlr.press/v202/li23an.html},
  abstract = 	 {In a vertical federated learning (VFL) system consisting of a central server and many distributed clients, the training data are vertically partitioned such that different features are privately stored on different clients. The problem of split VFL is to train a model split between the server and the clients. This paper aims to address two major challenges in split VFL: 1) performance degradation due to straggling clients during training; and 2) data and model privacy leakage from clients’ uploaded data embeddings. We propose FedVS to simultaneously address these two challenges. The key idea of FedVS is to design secret sharing schemes for the local data and models, such that information-theoretical privacy against colluding clients and curious server is guaranteed, and the aggregation of all clients’ embeddings is reconstructed losslessly, via decrypting computation shares from the non-straggling clients. Extensive experiments on various types of VFL datasets (including tabular, CV, and multi-view) demonstrate the universal advantages of FedVS in straggler mitigation and privacy protection over baseline protocols.},
  bibtex_show = {true},
  pdf={https://proceedings.mlr.press/v202/li23an/li23an.pdf},
  abbr={ICML 2023}
}

// Generalized Lagrange Coded Computing: A Flexible Computation-Communication Tradeoff
@INPROCEEDINGS{9834535,
  author={Zhu, Jinbao and Li, Songze},
  booktitle={2022 IEEE International Symposium on Information Theory (ISIT)}, 
  title={Generalized Lagrange Coded Computing: A Flexible Computation-Communication Tradeoff}, 
  year={2022},
  volume={},
  number={},
  pages={832-837},
  keywords={Privacy;Interpolation;Codes;Interference;Robustness;Encoding;Computational efficiency;coded distributed computing;Lagrange polynomial interpolation;interference cancellation;security and privacy},
  doi={10.1109/ISIT50566.2022.9834535},
  bibtex_show = {true},
  pdf={https://arxiv.org/pdf/2204.11168},
  abbr={ISIT2022}
  }

// Secure Federated Clustering
@misc{li2022securefederatedclustering,
      title={Secure Federated Clustering}, 
      author={Songze Li and Sizai Hou and Baturalp Buyukates and Salman Avestimehr},
      year={2022},
      eprint={2205.15564},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2205.15564}, 
      bibtex_show={true},
      pdf={https://arxiv.org/pdf/2205.15564},
      abbr={arXiv}
}

// Compressed Coded Distributed Computing
@ARTICLE{9336731,
  author={Elkordy, Ahmed Roushdy and Li, Songze and Maddah-Ali, Mohammad Ali and Avestimehr, A. Salman},
  journal={IEEE Transactions on Communications}, 
  title={Compressed Coded Distributed Computing}, 
  year={2021},
  volume={69},
  number={5},
  pages={2773-2783},
  keywords={Task analysis;Training;Distributed computing;Encoding;Bandwidth;Multicast communication;Machine learning;MapReduce;distributed training;gradient aggregation;coded multicasting},
  doi={10.1109/TCOMM.2021.3054906},
  bibtex_show = {true},
  pdf={https://arxiv.org/pdf/1805.01993},
  abbr={TCOM2021}
  }

// Architectures for coded mobile edge computing
@INPROCEEDINGS{8368530,
  author={Li, Songze and Maddah-Ali, Mohammad Ali and Avestimehr, A. Salman},
  booktitle={2017 IEEE Fog World Congress (FWC)}, 
  title={Architectures for coded mobile edge computing}, 
  year={2017},
  volume={},
  number={},
  pages={1-6},
  keywords={Computer architecture;Edge computing;Encoding;Computational modeling;Wireless communication;Load modeling;Automobiles},
  doi={10.1109/FWC.2017.8368530},
  bibtex_show = {true},
  abbr={FWC2017}
  }


// Poster Abstract: A Scalable Coded Computing Framework for Edge-Facilitated Wireless Distributed Computing
@INPROCEEDINGS{7774676,
  author={Li, Songze and Yu, Qian and Maddah-Ali, Mohammad Ali and Avestimehr, A. Salman},
  booktitle={2016 IEEE/ACM Symposium on Edge Computing (SEC)}, 
  title={Poster Abstract: A Scalable Coded Computing Framework for Edge-Facilitated Wireless Distributed Computing}, 
  year={2016},
  volume={},
  number={},
  pages={79-80},
  keywords={Wireless communication;Mobile communication;Encoding;Mobile computing;Uplink;Cloud computing;Wireless Distributed Computing;Scalable Distributed Computing;Edge-Facilitated Data Shuffling;MapReduce;Repetitive Computation Assignments;Coded Multicasting},
  doi={10.1109/SEC.2016.11},
  bibtex_show = {true},
  abbr={SEC 2016}
  }

// Rover-to-Orbiter Communication in Mars: Taking Advantage of the Varying Topology
@ARTICLE{7347383,
  author={Li, Songze and Kao, David T. H. and Avestimehr, A. Salman},
  journal={IEEE Transactions on Communications}, 
  title={Rover-to-Orbiter Communication in Mars: Taking Advantage of the Varying Topology}, 
  year={2016},
  volume={64},
  number={2},
  pages={572-585},
  keywords={Topology;Network topology;Mars;Transmitters;Encoding;Receivers;Earth;Mars;rover-to-orbiter communication;X-channel;varying topologies;coding across topologies;Mars;rover-to-orbiter communication;X-channel;varying topologies;coding across topologies},
  doi={10.1109/TCOMM.2015.2506157},
  bibtex_show = {true},
  pdf={https://arxiv.org/pdf/1504.04797},
  abbr={TCOM 2016}
  }

// Information-Theoretically Private Matrix Multiplication From MDS-Coded Storage
@ARTICLE{10054080,
  author={Zhu, Jinbao and Li, Songze and Li, Jie},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={Information-Theoretically Private Matrix Multiplication From MDS-Coded Storage}, 
  year={2023},
  volume={18},
  number={},
  pages={1680-1695},
  keywords={Servers;Privacy;Cryptography;Diseases;Hospitals;Computational modeling;Codes;Distributed matrix multiplication;data and index privacy;MDS-coded storage;colluding servers;polynomial secret sharing},
  doi={10.1109/TIFS.2023.3249565},
  bibtex_show = {true},
  pdf={https://arxiv.org/pdf/2205.01505},
  abbr={TIFS 2023}
  }

// TaiJi: Longest Chain Availability with BFT Fast Confirmation
@misc{li2020taijilongestchainavailability,
      title={TaiJi: Longest Chain Availability with BFT Fast Confirmation}, 
      author={Songze Li and David Tse},
      year={2020},
      eprint={2011.11097},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2011.11097}, 
      bibtex_show={true},
      pdf={https://arxiv.org/pdf/2011.11097},
      abbr={arXiv}
}

// Power allocation for Gaussian multiple access channel with noisy cooperative links
@INPROCEEDINGS{6853641,
  author={Li, Songze and Akyol, Emrah and Mitra, Urbashi},
  booktitle={2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Power allocation for Gaussian multiple access channel with noisy cooperative links}, 
  year={2014},
  volume={},
  number={},
  pages={474-478},
  keywords={Transmitters;Resource management;Noise measurement;Wireless sensor networks;Convex functions;Wireless communication;Educational institutions;multiple access channel;cooperation cost;power allocation;convex optimization},
  doi={10.1109/ICASSP.2014.6853641},
  bibtex_show = {true},
  abbr={ICASSP 2014}
  }

// Stochastic Coded Federated Learning: Theoretical Analysis and Incentive Mechanism Design
@ARTICLE{10336724,
  author={Sun, Yuchang and Shao, Jiawei and Mao, Yuyi and Li, Songze and Zhang, Jun},
  journal={IEEE Transactions on Wireless Communications}, 
  title={Stochastic Coded Federated Learning: Theoretical Analysis and Incentive Mechanism Design}, 
  year={2024},
  volume={23},
  number={6},
  pages={6623-6638},
  keywords={Servers;Training;Computational modeling;Performance evaluation;Data privacy;Data models;Noise level;Federated learning (FL);coded computing;straggler effect;mutual information differential privacy (MI-DP);incentive mechanism},
  doi={10.1109/TWC.2023.3334732},
  bibtex_show = {true},
  pdf={https://arxiv.org/pdf/2211.04132},
  abbr={TWC 2024}
}

// Jointly cooperative decode-and-forward relaying for secondary spectrum access
@INPROCEEDINGS{6310719,
  author={Songze Li and Mitra, Urbashi and Ratnam, Vishnu and Pandharipande, Ashish},
  booktitle={2012 46th Annual Conference on Information Sciences and Systems (CISS)}, 
  title={Jointly cooperative decode-and-forward relaying for secondary spectrum access}, 
  year={2012},
  volume={},
  number={},
  pages={1-6},
  keywords={Strontium;Decoding;Protocols;Transmitters;Markov processes;Receivers;Relays},
  doi={10.1109/CISS.2012.6310719},
  bibtex_show = {true},
  abbr={CISS 2012}
}

// Brief Announcement: Coded State Machine - Scaling State Machine Execution under Byzantine Faults
@inproceedings{10.1145/3293611.3331573,
author = {Li, Songze and Sahraei, Saeid and Yu, Mingchao and Avestimehr, Salman and Kannan, Sreeram and Viswanath, Pramod},
title = {Coded State Machine -- Scaling State Machine Execution under Byzantine Faults},
year = {2019},
isbn = {9781450362177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293611.3331573},
doi = {10.1145/3293611.3331573},
abstract = {We introduce Coded State Machine (CSM), an information-theoretic framework to securely and efficiently execute multiple state machines on Byzantine nodes. The standard method of solving this problem is using State Machine Replication, which achieves high security at the cost of low efficiency. CSM simultaneously achieves the optimal linear scaling in storage, throughput, and security with increasing network size. The storage is scaled via the design of Lagrange coded states and coded input commands that require the same storage size as their origins. The computational efficiency is scaled using a novel delegation algorithm, called INTERMIX, which is an information-theoretically verifiable matrix-vector multiplication algorithm of independent interest.},
booktitle = {Proceedings of the 2019 ACM Symposium on Principles of Distributed Computing},
pages = {150–152},
numpages = {3},
keywords = {verifiable computing, state machine execution, security, information theory, computational efficiency, byzantine faults},
location = {Toronto ON, Canada},
series = {PODC '19},
bibtex_show={true},
pdf={https://dl.acm.org/doi/pdf/10.1145/3293611.3331573},
abbr={PODC 2019}
}

// Communication-Efficient Coded Computing for Distributed Multi-Task Learning
@ARTICLE{10123015,
  author={Hu, Haoyang and Wu, Youlong and Shi, Yuanming and Li, Songze and Jiang, Chunxiao and Zhang, Wei},
  journal={IEEE Transactions on Communications}, 
  title={Communication-Efficient Coded Computing for Distributed Multi-Task Learning}, 
  year={2023},
  volume={71},
  number={7},
  pages={3861-3875},
  keywords={Uplink;Downlink;Servers;Task analysis;Encoding;Distributed databases;Computational modeling;Multi-task learning;coding;distributed learning;communication load},
  doi={10.1109/TCOMM.2023.3275194},
  bibtex_show = {true},
  abbr={TCOM 2023}
}

// Coded Distributed Computing for Hierarchical Multi-task Learning
@INPROCEEDINGS{10161632,
  author={Hu, Haoyang and Li, Songze and Cheng, Minquan and Wu, Youlong},
  booktitle={2023 IEEE Information Theory Workshop (ITW)}, 
  title={Coded Distributed Computing for Hierarchical Multi-task Learning}, 
  year={2023},
  volume={},
  number={},
  pages={480-485},
  keywords={Learning systems;Upper bound;Multitasking;Downlink;Topology;Servers;Relays;Multi-task learning;coded computing;distributed learning;hierarchical systems;communication load},
  doi={10.1109/ITW55543.2023.10161632},
  bibtex_show = {true},
  pdf={https://arxiv.org/pdf/2212.08236},
  abbr={ITW 2023}
  }

// Constructing Adversarial Examples for Vertical Federated Learning: Optimal Client Corruption through Multi-Armed Bandit
@inproceedings{
yao2024constructing,
title={Constructing Adversarial Examples for Vertical Federated Learning: Optimal Client Corruption through Multi-Armed Bandit},
author={Duanyi YAO and Songze Li and Ye XUE and Jin Liu},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=m52uU0dVbH},
bibtex_show={true},
pdf={https://openreview.net/pdf?id=m52uU0dVbH},
abbr={ICLR 2024}
}

// Generalized Lagrange Coded Computing: A Flexible Computation-Communication Tradeoff for Resilient, Secure, and Private Computation
@misc{zhu2023generalizedlagrangecodedcomputing,
      title={Generalized Lagrange Coded Computing: A Flexible Computation-Communication Tradeoff for Resilient, Secure, and Private Computation}, 
      author={Jinbao Zhu and Hengxuan Tang and Songze Li and Yijia Chang},
      year={2023},
      eprint={2204.11168},
      archivePrefix={arXiv},
      primaryClass={cs.IT},
      url={https://arxiv.org/abs/2204.11168}, 
      bibtex_show={true},
      pdf={https://arxiv.org/pdf/2204.11168},
      abbr={arXiv}
}

// Fully Privacy-Preserving Federated Representation Learning via Secure Embedding Aggregation
@misc{cryptoeprint:2022/784,
      author = {Jiaxiang Tang and Jinbao Zhu and Songze Li and Kai Zhang and Lichao Sun},
      title = {Fully Privacy-Preserving Federated Representation Learning via Secure Embedding Aggregation},
      howpublished = {Cryptology ePrint Archive, Paper 2022/784},
      year = {2022},
      note = {\url{https://eprint.iacr.org/2022/784}},
      url = {https://eprint.iacr.org/2022/784},
      bibtex_show={true},
      pdf={https://eprint.iacr.org/2022/784.pdf},
}

// Cooperative spectrum sharing with joint receiver decoding
@INPROCEEDINGS{6638674,
  author={Li, Songze and Mitra, Urbashi and Pandharipande, Ashish},
  booktitle={2013 IEEE International Conference on Acoustics, Speech and Signal Processing}, 
  title={Cooperative spectrum sharing with joint receiver decoding}, 
  year={2013},
  volume={},
  number={},
  pages={5298-5302},
  keywords={Decoding;Joints;Noise;Receivers;Protocols;Cognitive radio;Radio transmitters;spectrum sharing;cognitive radio;joint decoding;outage performance;virtual MIMO system},
  doi={10.1109/ICASSP.2013.6638674},
  bibtex_show = {true},
  abbr={ICASSP 2013}
  }

// BackdoorIndicator: Leveraging OOD Data for Proactive Backdoor Detection in Federated Learning
@article{DBLP:journals/corr/abs-2405-20862,
  author       = {Songze Li and
                  Yanbo Dai},
  title        = {BackdoorIndicator: Leveraging {OOD} Data for Proactive Backdoor Detection
                  in Federated Learning},
  journal      = {CoRR},
  volume       = {abs/2405.20862},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2405.20862},
  doi          = {10.48550/ARXIV.2405.20862},
  eprinttype    = {arXiv},
  eprint       = {2405.20862},
  timestamp    = {Mon, 24 Jun 2024 10:16:41 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2405-20862.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  bibtex_show={true},
  pdf={https://arxiv.org/pdf/2405.20862},
  abbr={USENIX Security 2024}
}

// Towards Client Driven Federated Learning
@article{DBLP:journals/corr/abs-2405-15407,
  author       = {Songze Li and
                  Chenqing Zhu},
  title        = {Towards Client Driven Federated Learning},
  journal      = {CoRR},
  volume       = {abs/2405.15407},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2405.15407},
  doi          = {10.48550/ARXIV.2405.15407},
  eprinttype    = {arXiv},
  eprint       = {2405.15407},
  timestamp    = {Wed, 19 Jun 2024 08:52:55 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2405-15407.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  bibtex_show={true},
  pdf={https://arxiv.org/pdf/2405.15407}, 
  abbr={arXiv}
}

// Identity Inference from CLIP Models using Only Textual Data
@article{DBLP:journals/corr/abs-2405-14517,
  author       = {Songze Li and
                  Ruoxi Cheng and
                  Xiaojun Jia},
  title        = {Identity Inference from {CLIP} Models using Only Textual Data},
  journal      = {CoRR},
  volume       = {abs/2405.14517},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2405.14517},
  doi          = {10.48550/ARXIV.2405.14517},
  eprinttype    = {arXiv},
  eprint       = {2405.14517},
  timestamp    = {Wed, 19 Jun 2024 08:52:51 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2405-14517.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  bibtex_show={true},
  pdf={https://arxiv.org/pdf/2405.14517},
  abbr={arXiv}
}

// Leveraging Label Information for Stealthy Data Stealing in Vertical Federated Learning
@article{DBLP:journals/corr/abs-2404-19582,
  author       = {Duanyi Yao and
                  Songze Li and
                  Xueluan Gong and
                  Sizai Hou and
                  Gaoning Pan},
  title        = {Leveraging Label Information for Stealthy Data Stealing in Vertical
                  Federated Learning},
  journal      = {CoRR},
  volume       = {abs/2404.19582},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2404.19582},
  doi          = {10.48550/ARXIV.2404.19582},
  eprinttype    = {arXiv},
  eprint       = {2404.19582},
  timestamp    = {Mon, 27 May 2024 14:55:45 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2404-19582.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  bibtex_show={true},
  pdf={https://arxiv.org/pdf/2404.19582},
  abbr={arXiv}
}

// FedMeS: Personalized Federated Continual Learning Leveraging Local Memory
@article{DBLP:journals/corr/abs-2404-12710,
  author       = {Jin Xie and
                  Chenqing Zhu and
                  Songze Li},
  title        = {FedMeS: Personalized Federated Continual Learning Leveraging Local
                  Memory},
  journal      = {CoRR},
  volume       = {abs/2404.12710},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2404.12710},
  doi          = {10.48550/ARXIV.2404.12710},
  eprinttype    = {arXiv},
  eprint       = {2404.12710},
  timestamp    = {Wed, 22 May 2024 08:50:15 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2404-12710.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  bibtex_show={true},
  pdf={https://arxiv.org/pdf/2404.12710},
  abbr={arXiv}
}

// OmniLytics+: A Secure, Efficient, and Affordable Blockchain Data Market for Machine Learning through Off-Chain Processing
@misc{li2024omnilyticssecureefficientaffordable,
      title={OmniLytics+: A Secure, Efficient, and Affordable Blockchain Data Market for Machine Learning through Off-Chain Processing}, 
      author={Songze Li and Mingzhe Liu and Mengqi Chen},
      year={2024},
      eprint={2406.06477},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2406.06477}, 
      bibtex_show={true},
      pdf={https://arxiv.org/pdf/2406.06477},
      abbr={arXiv}
}

// On Exploiting Network Topology for Hierarchical Coded Multi-task Learning
@ARTICLE{10478954,
  author={Hu, Haoyang and Li, Songze and Cheng, Minquan and Ma, Shuai and Shi, Yuanming and Wu, Youlong},
  journal={IEEE Transactions on Communications}, 
  title={On Exploiting Network Topology for Hierarchical Coded Multi-task Learning}, 
  year={2024},
  volume={},
  number={},
  pages={1-1},
  keywords={Relays;Servers;Task analysis;Distance learning;Computer aided instruction;Downlink;Uplink;Multi-task learning;coding techniques;distributed learning;hierarchical systems;communication load},
  doi={10.1109/TCOMM.2024.3381671},
  bibtex_show={true},
  abbr={TCOM 2024}
  }

// Active Few-shot Learning For RouteNet-Fermi
@inproceedings{10.1145/3630049.3630174,
author = {Fu, Xiaoyi and Shao, Menghan and Li, Songze and Yang, Liuqing},
title = {Active Few-shot Learning For RouteNet-Fermi},
year = {2023},
isbn = {9798400704482},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630049.3630174},
doi = {10.1145/3630049.3630174},
abstract = {Machine-learning-based Network Modeling requires a compact training data set that contains diversified network topology and configurations covering different congestion levels. We formalize the problem of modeling network delay using Multi-stage Message Passing Graph Neural Networks (GNNs) under the constraints of a limited number of training samples and a limited number of nodes for the topology of each sample as a few-shot learning problem. To tackle it, we propose an active learning algorithm that selectively randomizes initial features that are invariant of node numbers and then uses pool-based uncertainty sampling for selecting the approximated optimal network topology based on the Shannon entropy. The approximation could be theoretically proven and confirmed with experimental results.},
booktitle = {Proceedings of the 2nd on Graph Neural Networking Workshop 2023},
pages = {19–24},
numpages = {6},
keywords = {active learning, few-shot learning, graph neural networks},
location = {Paris, France},
series = {GNNet '23},
bibtex_show={true},
abbr={NeurIPS Workshop 2023}
}

// Multi-Server Secure Aggregation with Unreliable Communication Links
@INPROCEEDINGS{10437745,
  author={Liang, Kai and Li, Songze and Ding, Ming and Wu, Youlong},
  booktitle={GLOBECOM 2023 - 2023 IEEE Global Communications Conference}, 
  title={Multi-Server Secure Aggregation with Unreliable Communication Links}, 
  year={2023},
  volume={},
  number={},
  pages={2560-2565},
  keywords={Threat modeling;Training;Redundancy;Downlink;Security;Channel coding;Uplink;Coding computing;federated learning;straggling links;secure aggregation},
  doi={10.1109/GLOBECOM54140.2023.10437745},
  bibtex_show={true},
  pdf={https://arxiv.org/pdf/2304.07573},
  abbr={GLOBECOM 2023}
  }

// Secure Gradient Aggregation for Wireless Multi-Server Federated Learning
@INPROCEEDINGS{10206522,
  author={Huang, Zhenhao and Li, Songze and Liang, Kai and Wu, Youlong},
  booktitle={2023 IEEE International Symposium on Information Theory (ISIT)}, 
  title={Secure Gradient Aggregation for Wireless Multi-Server Federated Learning}, 
  year={2023},
  volume={},
  number={},
  pages={2404-2409},
  keywords={Wireless communication;Wireless sensor networks;Federated learning;Downlink;Encoding;Servers;Communication system security},
  doi={10.1109/ISIT54713.2023.10206522},
  bibtex_show={true},
  abbr={ISIT 2023}
}

// Secure Embedding Aggregation for Federated Representation Learning
@INPROCEEDINGS{10206974,
  author={Tang, Jiaxiang and Zhu, Jinbao and Li, Songze and Sun, Lichao},
  booktitle={2023 IEEE International Symposium on Information Theory (ISIT)}, 
  title={Secure Embedding Aggregation for Federated Representation Learning}, 
  year={2023},
  volume={},
  number={},
  pages={2392-2397},
  keywords={Representation learning;Privacy;Protocols;Social networking (online);Distributed databases;Complexity theory;Servers},
  doi={10.1109/ISIT54713.2023.10206974},
  bibtex_show={true},
  pdf={https://arxiv.org/pdf/2206.09097},
  abbr={ISIT 2023}
  }
